{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Titanic Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal of this project is to apply the tools of machine learning to predict which passengers survived the Titanic tragedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the needed libraries and import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pandas import read_csv,get_dummies,to_numeric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"Importing the Classification Algorithms\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\"\"\"Importing Ensemble Algorithms\"\"\"\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "filename_train = 'train.csv'\n",
    "Dataset = read_csv(filename_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Understand the Data first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to predict if a person survives based on some features\n",
    "The features are:\n",
    "    \n",
    "- Survived: Survived (1) or died (0)\n",
    "- Pclass: Passenger’s class\n",
    "- Name: Passenger’s name\n",
    "- Sex: Passenger’s sex\n",
    "- Age: Passenger’s age\n",
    "- SibSp: Number of siblings/spouses aboard\n",
    "- Parch: Number of parents/children aboard\n",
    "- Ticket: Ticket number\n",
    "- Fare: Fare\n",
    "- Cabin: Cabin\n",
    "- Embarked: Port of embarkation\n",
    "\n",
    "We can start with some expectations\n",
    "- I expect Females have a more chance to survive than men, so sex has an impact\n",
    "- I expect Children have more chance to survive than adults, so Age matters\n",
    "- I expect wealthy people to have more chance to survive, Pclass matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_Values_Table = Dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first focus on the Age, based on my expectation I do not really care about the exact age, all I need to know is if the individual is a kid or an adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a new column based on age and splitit in 2 categories child, adult\n",
    "- child 0-18\n",
    "- adult 19-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['Age'] = to_numeric(Dataset['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in Dataset['Age']:\n",
    "    if item<19:\n",
    "        Dataset['Age'] = Dataset['Age'].replace(item,'Child')\n",
    "    elif item>=19:\n",
    "        Dataset['Age'] = Dataset['Age'].replace(item,'Adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child    139\n",
      "Adult    575\n",
      "Name: Age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Dataset['Age'].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Missing_Values_Table = Dataset.isnull().sum()\n",
    "print(Missing_Values_Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then to fill the NaN values of Age, if there is Mr or Mrs in the name it's an adult othewise it's a child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMr = Dataset[Dataset['Name'].str.contains(\"Mr.\")]\n",
    "Dataset['Age'] = dataMr['Age'].fillna('Adult') \n",
    "\n",
    "dataMrs = Dataset[Dataset['Name'].str.contains(\"Mrs.\")] \n",
    "Dataset['Age'] = dataMrs['Age'].fillna('Adult') \n",
    "\n",
    "Dataset['Age'] = Dataset['Age'].fillna('Child')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would Argue that there are some columns that can be dropped because they have no impact whatso ever on the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset.drop(['PassengerId'], axis=1)\n",
    "\n",
    "Dataset = Dataset.drop(['Name'], axis=1)\n",
    "\n",
    "Dataset = Dataset.drop(['Ticket'], axis=1)\n",
    "\n",
    "Dataset = Dataset.drop(['Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "Dataset['Sex']=encoder.fit_transform(Dataset['Sex'])\n",
    "\n",
    "Dataset['Age']=encoder.fit_transform(Dataset['Age'])\n",
    "\n",
    "Dataset = get_dummies(Dataset, columns=['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Dataset.values\n",
    "X = array[:,1:]\n",
    "Y = array[:,0]\n",
    "\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n",
    "        test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a bunch of algorithms and measure the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 0.8062793427230048 0.041563591676032616\n",
      "ScaledNB: 0.7879499217527387 0.055264119093866436\n",
      "ScaledKNN: 0.8091353677621284 0.056965809921960216\n",
      "ScaledCART: 0.7781690140845071 0.049851857703379195\n",
      "ScaledSVC: 0.823180751173709 0.054511198348989924\n"
     ]
    }
   ],
   "source": [
    "#Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\n",
    "LogisticRegression(solver = 'liblinear'))])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',\n",
    "GaussianNB())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\n",
    "KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\n",
    "DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()),('SVC', SVC())])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {cv_results.mean()} {cv_results.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the SVM algorithm has the best performance, Let's apply some tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8258426966292135 using {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 0.1, 'kernel': 'linear'}\n",
      "0.7752808988764045 (0.05567158960480092) with: {'C': 0.1, 'kernel': 'poly'}\n",
      "0.8132022471910112 (0.04493118019579273) with: {'C': 0.1, 'kernel': 'rbf'}\n",
      "0.797752808988764 (0.04401759072585399) with: {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 0.3, 'kernel': 'linear'}\n",
      "0.7837078651685393 (0.04633845605512466) with: {'C': 0.3, 'kernel': 'poly'}\n",
      "0.8230337078651685 (0.05035188452116672) with: {'C': 0.3, 'kernel': 'rbf'}\n",
      "0.7401685393258427 (0.05053517104299552) with: {'C': 0.3, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 0.5, 'kernel': 'linear'}\n",
      "0.7921348314606742 (0.04117316762672349) with: {'C': 0.5, 'kernel': 'poly'}\n",
      "0.8258426966292135 (0.0499296578863745) with: {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.6980337078651685 (0.05800423665434465) with: {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 0.7, 'kernel': 'linear'}\n",
      "0.7949438202247191 (0.04410404804103409) with: {'C': 0.7, 'kernel': 'poly'}\n",
      "0.8230337078651685 (0.05211616083267014) with: {'C': 0.7, 'kernel': 'rbf'}\n",
      "0.6882022471910112 (0.05616097960626646) with: {'C': 0.7, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 0.9, 'kernel': 'linear'}\n",
      "0.8061797752808989 (0.044229857768487965) with: {'C': 0.9, 'kernel': 'poly'}\n",
      "0.8230337078651685 (0.05211616083267014) with: {'C': 0.9, 'kernel': 'rbf'}\n",
      "0.6867977528089888 (0.05098109005839872) with: {'C': 0.9, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 1.0, 'kernel': 'linear'}\n",
      "0.8132022471910112 (0.04521158542618964) with: {'C': 1.0, 'kernel': 'poly'}\n",
      "0.8216292134831461 (0.05492546597291916) with: {'C': 1.0, 'kernel': 'rbf'}\n",
      "0.6882022471910112 (0.05508168698634749) with: {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 1.3, 'kernel': 'linear'}\n",
      "0.8089887640449438 (0.052765782407536696) with: {'C': 1.3, 'kernel': 'poly'}\n",
      "0.8230337078651685 (0.054605606357260084) with: {'C': 1.3, 'kernel': 'rbf'}\n",
      "0.6825842696629213 (0.054956453607739014) with: {'C': 1.3, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 1.5, 'kernel': 'linear'}\n",
      "0.8103932584269663 (0.05277025778623446) with: {'C': 1.5, 'kernel': 'poly'}\n",
      "0.8258426966292135 (0.05284356486688458) with: {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.6853932584269663 (0.05940987047989014) with: {'C': 1.5, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 1.7, 'kernel': 'linear'}\n",
      "0.8103932584269663 (0.05277025778623446) with: {'C': 1.7, 'kernel': 'poly'}\n",
      "0.824438202247191 (0.055303344795068966) with: {'C': 1.7, 'kernel': 'rbf'}\n",
      "0.6853932584269663 (0.05940987047989014) with: {'C': 1.7, 'kernel': 'sigmoid'}\n",
      "0.8019662921348315 (0.043292918282728514) with: {'C': 2.0, 'kernel': 'linear'}\n",
      "0.8132022471910112 (0.053781955836409566) with: {'C': 2.0, 'kernel': 'poly'}\n",
      "0.8230337078651685 (0.0560089541340276) with: {'C': 2.0, 'kernel': 'rbf'}\n",
      "0.6853932584269663 (0.05919443776975209) with: {'C': 2.0, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# svm Algorithm tuning\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "C_values = numpy.array([0.1,0.3,0.5,0.7,0.9,1.0,1.3,1.5,1.7,2.0])\n",
    "kernel_values = ['linear','poly','rbf','sigmoid']\n",
    "param_grid = dict(C=C_values, kernel=kernel_values)\n",
    "model = SVC(gamma='auto')\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"{mean} ({stdev}) with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Prediction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7541899441340782\n",
      "[[89 21]\n",
      " [23 46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.81      0.80       110\n",
      "         1.0       0.69      0.67      0.68        69\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       179\n",
      "   macro avg       0.74      0.74      0.74       179\n",
      "weighted avg       0.75      0.75      0.75       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare the model\n",
    "model = SVC(gamma='auto',C=0.5, kernel = 'rbf')\n",
    "model.fit(X_train, Y_train)\n",
    "# estimate accuracy on validation dataset\n",
    "predictions = model.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
